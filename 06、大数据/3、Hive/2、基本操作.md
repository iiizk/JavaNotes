[TOC]



# 基础指令整理

| 命令                                                         | 作用                                         | 额外说明                                                     |
| ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| show databases;                                              | 查看都有哪些数据库                           |                                                              |
| create database park;                                        | 创建park数据库                               | 创建的数据库，实际是在Hadoop的HDFS文件系统里创建一个目录节点，统一存在： /user/hive/warehouse 目录下 |
| use park;                                                    | 进入park数据库                               |                                                              |
| show tables;                                                 | 查看当前数据库下所有表                       |                                                              |
| create table stu   (id int,name string);                     | 创建stu表，以及相关的两个字段                | hive里，表示字符串用的是string，不用char和varchar    所创建的表，也是HDFS里的一个目录节点 |
| insert into stu values(1,'zhang')                            | 向stu表插入数据                              | HDFS不支持数据的修改和删除，因此已经插入的数据不能够再进行任何的改动    在Hadoop2.0版本后支持了数据追加。实际上，insert into 语句执行的是追加操作    hive支持查询，行级别的插入。不支持行级别的删除和修改    hive的操作实际是执行一个job任务，调用的是Hadoop的MR    插入完数据之后，发现HDFS stu目录节点下多了一个文件，文件里存了插入的数据，因此，hive存储的数据，是通过HDFS的文件来存储的。 |
| select * from stu                                            | 查看表数据                                   | 也可以根据字段来查询，比如select id from stu                 |
| drop table stu                                               | 删除表                                       |                                                              |
| select * from stu                                            | 查询stu表数据                                |                                                              |
| load data local inpath '/home/software/1.txt' into table stu; | 通过加载文件数据到指定的表里                 | 在执行完这个指令之后，发现hdfs stu目录下多了一个1.txt文件。由此可见，hive的工作原理实际上就是在管理hdfs上的文件，把文件里数据抽象成二维表结构，然后提供hql语句供程序员查询文件数据    可以做这样的实验：不通过load 指令，而通过插件向stu目录下再上传一个文件，看下hive是否能将数据管理到stu表里。 |
| create table stu1(id int,name string) row format delimited fields  terminated by '   '; | 创建stu1表，并指定分割符 空格。              |                                                              |
| desc stu                                                     | 查看 stu表结构                               |                                                              |
| create table stu2 like stu                                   | 创建一张stu2表，表结构和stu表结构相同        | like只复制表结构，不复制数据                                 |
| insert overwrite table stu2   select * from stu              | 把stu表数据插入到stu2表中                    |                                                              |
| insert  overwrite local directory '/home/stu' row format delimited fields terminated by ' ' select * from  stu; | 将stu表中查询的数据写到本地的/home/stu目录下 |                                                              |
| insert  overwrite directory '/stu' row format delimited fields terminated by '   ' select  * from stu; | 将stu表中查询的数据写到HDFS的stu目录下       |                                                              |
| from stu insert overwrite table stu1 select * insert overwrite table stu2 select *; | 将stu表中查询的数据写到stu1以及stu2两张表中  |                                                              |
| alter table stu rename to  stu2                              | 为表stu重命名为stu2                          |                                                              |
| alter table stu  add columns (age int);                      | 为表stu增加一个列字段age，类型为int          |                                                              |
| exit                                                         | 退出hive                                     |                                                              |



# 基本命令

1. 将`/home/hivedata/person.txt`文件加载到Hive的person表中

   ```mysql
   load data local inpath '/home/hivedata/person.txt' into table person;
   ```

2. 建表的时候指定字段之间的间隔字符是：**空格**

   ```mysql
   create table person (id int,name string,age int) row format delimited fields terminated by ' ';
   ```

3. 建立一张和person结构一样的表p2

   ```mysql
   create table p2 like person
   ```

4. 将person表中的`age>=14`的数据查询出来放到p2中

   ```mysql
   insert into table p2 select * from person where age>=14;
   ```

5. 将person表中`age>=14`的数据查询出来放到p2中，同时将`id<3`的数据查询出来放到p3表中

   ```mysql
   from person insert into table p2 select * where age>=14
   	insert into table p3 select * where id<3
   ```

6. 将person表中的数据查询出来写到本地目录中，并且在本地目录中，字段之间需要用`,`作为间隔

   ```mysql
   insert overwrite local directory '/home/hivedata' row format delimited fields terminated by ','
   	select * from person where id>=3
   ```

7. 将person表中的数据查询出来放到HDFS的指定目录下

   ```mysql
   insert overwrite directory '/person' row format delimited fields terminated by ' '
   	select * form person;
   ```

8. 将person表改名为p1

   ```mysql
   alter table person rename to p1;
   ```

9. 向p1表中添加一个字段

   ```mysql
   alter table p1 add columns (gender string);
   ```


> 建表基本类型需要指定长度

```sql
CREATE TABLE
IF
	NOT EXISTS jczc996_ods_inc.pc_gt_ndsj_t (
		id VARCHAR ( 20 ),
		bgq VARCHAR ( 20 ),
		bgq_code VARCHAR ( 50 ),
		bgqb VARCHAR ( 20 ),
		bgqb_code VARCHAR ( 20 ),
		dq VARCHAR ( 20 ),
		dq_code VARCHAR ( 20 ),
		tjsx VARCHAR ( 20 ),
		tjsx_code VARCHAR ( 20 ),
		zb VARCHAR ( 100 ),
		zb_code VARCHAR ( 20 ),
		zbz double,
		jldw VARCHAR ( 20 ),
		fl_dl VARCHAR ( 100 ),
		fl_xl VARCHAR ( 255 ),
		sjly VARCHAR ( 50 ),
		sjfl VARCHAR ( 2 ),
		rksj TIMESTAMP,
        zbjx_id VARCHAR ( 50 ) 
	);
```



# 常见表结构

## 内部表和外部表

1. 内部表：

   - 在Hive中手动创建，手动添加数据的表
   - 再删除内部表的时候，在HDFS上对应的目录也会被删除

2. 外部表：

   - 在Hive中建立的管理HDFS中已经存在的数据的表

   - 在删除外部表的时候，在HDFS上对应的目录不会被删除

   > 外部表的建表语句：关键字：`external`

   ```mysql
   create external table 
   	score (name string, chinese int, math int, english int) 
   	row format delimited fields terminated by ' ' 
   	location '/score'; 
   ```

   

## 分区表

> 分区表的作用是对数据进行分类

1. 建表语句

   ```mysql
   create table cities(id int, name string) 
       partitioned by(province string) 
       row format delimited fields terminated by ' '; 
   ```

2. 加载数据

   ```mysql
   load data local inpath '/home/hivedata/hebei.txt' into table cities partition(province='hebei');
   load data local inpath '/home/hivedata/jiangsu.txt' into table cities partition (province= 'jiangsu'); 
   ```

3. ==每一个分区在HDFS上对应一个目录==

   ![](https://gitee.com/sxhDrk/images/raw/master/imgs/多级分类.png)

4. 在建立分区表的时候，分区字段在要求在原始数据中不能存在，而是在加载数据的时候手动指定

5. 如果手动在HDFS上新建了一个目录作为分区，那么需要在Hive中添加这个分区

   ```mysql
   alter table cities add partition (province='shandong') 
   	location '/user/hive/warehouse/hivedemo.db/cities/province=shandong'; 
   ```

   或者利用下述命令来修复分区。但是这个命令不稳定，有时候会修复失败

   ```mysql
   msck repair table cities; 
   ```

6. 删除分区

   ```mssql
   alter table cities drop partition(province='guangdong'); 
   ```

7. 修改分区名

   ```mysql
   alter table cities partition(province='jiangxi') rename to partition(province='jiangsu'); 
   ```

8. 在分区表中

   - 如果按照分区字段进行查询，会大幅度的提高查询效率，因为此时只需要查询对应目录即可，而不需要将整个表全部查询
   - 如果查询的时候进行了跨分区的查询，此时查询效率反而会变低，因为需要读取多个文件

9. 动态分区：将数据从未分区的表中向已分区的表中存放

   - 建表管理源数据

     ```mysql
     create table c_tmp(cid int, cname string, cpro string)
     	row format delimited fields terminated by ' '; 
     ```

   - 加载数据

     ```mysql
     load data local inpath '/home/hivedata/city.txt' into table c_tmp; 
     ```

   - 开启动态分区，动态分区默认是严格模式，不开启

     ```mysql
     set hive.exec.dynamic.partition.mode=nonstrict; 
     ```

   - 动态分区

     ```mysql
     insert into table cities partition(province) 
     	select cid, cname, cpro from c_tmp distribute by(cpro); 
     ```

10. 在Hive中，分区字段可以有多个，往往是用于对数据进行==多级分区==的。写在前面的分区表示大类，包含后面的分区

    ```mysql
    create table product(id int, name string) 
    	partitioned by(kind string, subkind string, grandkind string) 
    	row format delimited fields terminated by ' ';
    
    load data local inpath '/home/hivedata/shoes.txt'into table product 
    	partition (kind='clothes',subkind='shoes',grandkind='male');  
    ```

    ![](https://gitee.com/sxhDrk/images/raw/master/imgs/分桶表.png)



## 分桶表

1. 作用：对数据进行抽样
2. 分桶表的特点在于：不支持load加载方式，而是需要从其他表中查询然后放到分桶表中

> 案例

- 建表：表示根据name字段来进行分桶，将数据分到6个桶中

  > 先计算name字段值的哈希吗，然后利用这个哈希吗对桶进行取余，取余完成之后决定放到哪个桶中

  ```mysql
  create table score_sample(name string, chinese int, math int, english int) 
      clustered by (name) into 6 buckets 
      row format delimited fields terminated by ' '; 
  ```

- 开启分桶机制

  ```mysql
  set hive.enforce.bucketing = true; 
  ```

- 创建外部表

  ```mysql
  create external table score(name string, chinese int, math int, english int) 
  	row format delimited fields terminated by ' ' location '/score';
  ```

- 将数据从`score`表中查询出来放到`score_sample`表中，这个过程会自动进行分桶

  ```mysql
  insert overwrite table score_sample select * from score;  
  ```

  > 分桶表文件名从0开始

  ![](https://gitee.com/sxhDrk/images/raw/master/imgs/分区在HDFS上对应一个目录.png)

- 抽样：`bucket x out of y`---x表示起始桶编号（图片的000001_0，编号是2），y表示抽样步长

  > `bucket 1 out of 3`表示从第一个桶（对应文件名为0）中抽取，每隔3个桶抽一次

  ```mysql
  select * from score_sample tablesample (bucket 1 out of 3 on name); 
  ```

  





# SerDe机制

1. SerDe（Serializar/Deserializar）是Hive提供的一套序列化/反序列化机制
2. 实际开发中，SerDe机制专门用于处理不规则的数据

> 案例：把Tomcat日志整理到表中

1. 日志信息

   ```
   192.168.120.23 -- [30/Apr/2018:20:25:32 +0800] "GET /asf.avi HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:32 +0800] "GET /bupper.png HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:32 +0800] "GET /bupper.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /bg-button HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /bbutton.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /asf.jpg HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /tomcat.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /tomcat.png HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /tbutton.png HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /tinput.png HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:33 +0800] "GET /tbg.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:34 +0800] "GET /tomcat.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:34 +0800] "GET /bg.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:34 +0800] "GET /bg-button.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:34 +0800] "GET /bg-input.css HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:34 +0800] "GET /bd-input.png HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:34 +0800] "GET /bg-input.png HTTP/1.1" 304 -
   192.168.120.23 -- [30/Apr/2018:20:25:34 +0800] "GET /music.mp3 HTTP/1.1" 304 -
   ```

   

2. 建表

   ```mysql
   create table logs(ip string, time string, timezone string, request_way string, resource string, protocols string, statusid int) 
       row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' 
       with serdeproperties ("input.regex" = "(.*) \-\- \\[(.*) (.*)\\] \"(.*) (.*) (.*)\" ([0-9]+) \-") 
       stored as textfile; 
   ```

   

3. 加载数据

   ```mysql
   load data local inpath '/home/hivedata/logs.txt' into table logs; 
   ```

   



# join

1. 在Hive中，提供了inner/left/right/full outer/left semi join 查询

2. 如果join的时候，没有明确指定形式，那么默认就是inner join

3. `products p left semi join orders o`表示查询products表中有哪些数据在orders表中出现过

   

> 案例

1. 建立外部表orders

   ```mysql
   create external table orders(orderid int, orderdate string, proid int, num int) 
   	row format delimited fields terminated by ' ' location '/order';
   ```

2. 建立外部表products

   ```mysql
   create external table products (proid int, name string, price double) 
   	row format delimited fields terminated by ' ' location '/product';
   ```

3. 关联查询

   ```mysql
   select * from products p left semi join orders o on p.proid = o.proid;
   ```

   





# beeline

1. beeline是Hive提供的一种用于远程连接Hive的方式，其提供了表格形式(类似MySQL)来进行展现

2. 如果使用这种方式，需要先后台启动`hiveserver2`

   `sh hive --service hiveserver2 &`

3. 远程连接：`beeline -u jdbc:hive2://hadoop01:10000/hivedemo`

   - 注意：此种方式的情况下，默认是不对用户名和密码进行验证的
   - 指定验证用户：`beeline -u jdbc:hive2://hadoop01:10000/hivedemo -n root`

   





# 视图

1. 在一个表中包含了很多字段，但是每一个字段的使用频率并不相同，此时可以考虑将使用频率高的字段抽取出来形成一个子表，又希望子表能与主表自动关联，此时这个子表可以以视图的形式来体现
2. **虚拟视图**：抽取出来的视图放到了内存中
3. **物化视图**：抽取出来的视图放到了磁盘中
4. Hive中只支持虚拟视图，不支持物化视图
5. **视图封装的select语句在封装时并没有执行，在第一次使用视图的时候才会触发select**

> 案例

- 建立视图

  ```mysql
  create view o_view as select orderid, num from orders;
  ```

- 查询语句

  ```mysql
  select orderid from o_view;
  ```

- 删除视图

  ```mysql
  drop view o_view;
  ```

  





# 索引

1. 在数据库中，因为在建表的时候存在主键，所以会自动根据主键建立索引。在Hive中，没有主键的概念，所以也不会自动建立索引
2. 在Hive中可以针对任何一个字段来建立索引，而且一张表可以存放多个索引

> 案例

- 建立索引表

  ```mysql
  create index oin on table orders(orderid) 
      as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'  
      with deferred rebuild in table o_index;
  ```

- 手动建立索引

  ```mysql
  alter index oin on orders rebuild;
  ```

- 删除索引

  ```mysql
  drop index oin on orders;
  ```

  