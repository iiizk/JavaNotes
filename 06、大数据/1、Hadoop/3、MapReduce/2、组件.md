[TOC]

# Writable - 序列化

## 概述

1. 在Hadoop的集群工作过程中，一般是利用==RPC==来进行集群节点之间的通信和消息的传输，所以要求MapReduce处理的对象必须可以==进行序列化/反序列==操作
2. Hadoop并没有使用Java原生的序列化，而是利用的是==Avro==实现的序列化和反序列，并且在此基础上进行了更好的封装，提供了便捷的API
3. 在Hadoop中要求被序列化的对象对应的类必须实现`Writable`接口，重写其中的`write`方法以及`readFields`方法
4. 序列化过程中要求==属性值不能为null==



## 案例

> 统计每个人的总流量，源文件：`flow.txt`

```
13877779999 bj zs 2145
13766668888 sh ls 1028
13766668888 sh ls 9987
13877779999 bj zs 5678
13544445555 sz ww 10577
13877779999 sh zs 2145
13766668888 sh ls 9987
```



### Flow类

```java
// 实现Hadoop提供的序列化接口
public class Flow implements Writable {
    private String phone = "";
    private String address = "";
    private String name = "";
    private int flow;
 
    public String getPhone() {
        return phone;
    }
 
    public void setPhone(String phone) {
        this.phone = phone;
    }
 
    public String getAddress() {
        return address;
    }
 
    public void setAddress(String address) {
        this.address = address;
    }
 
    public String getName() {
        return name;
    }
 
    public void setName(String name) {
        this.name = name;
    }
 
    public int getFlow() {
        return flow;
    }
 
    public void setFlow(int flow) {
        this.flow = flow;
    }
 
    // 序列化
    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(phone);
        out.writeUTF(address);
        out.writeUTF(name);
        out.writeInt(flow);
    }
 
    // 反序列化
    @Override
    public void readFields(DataInput in) throws IOException {
        phone = in.readUTF();
        address = in.readUTF();
        name = in.readUTF();
        flow = in.readInt();
    }
}
```



### Mapper类

```java
public class SerialFlowMapper extends Mapper<LongWritable, Text, Text, Flow> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        // 13877779999 bj zs 2145
        String[] arr = value.toString().split(" ");
        Flow f = new Flow();
        f.setPhone(arr[0]);
        f.setCity(arr[1]);
        f.setName(arr[2]);
        f.setFlow(Integer.parseInt(arr[3]));
        context.write(new Text(f.getName()), f);
    }
}
```



### Reducer类

```java
// 统计每一个人花费的总流量 - Text, IntWritable
// 统计每一个人出现的城市 - Text, Text
// 统计每一个人拥有的手机号 - Text, Text
public class SerialFlowReducer extends Reducer<Text, Flow, Text, IntWritable> {
    @Override
    protected void reduce(Text key, Iterable<Flow> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (Flow val : values) {
            sum += val.getFlow();
        }
        context.write(key, new IntWritable(sum));
    }
}
```



### Driver类

```java
public class SerialFlowDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(SerialFlowDriver.class);
        job.setMapperClass(SerialFlowMapper.class);
        job.setReducerClass(SerialFlowReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Flow.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job,
                new Path("hdfs://hadoop01:9000/txt/flow.txt"));
        FileOutputFormat.setOutputPath(job,
                new Path("hdfs://hadoop01:9000/result/serialflow"));
        job.waitForCompletion(true);

    }
}
```



### 结果

```
ls	21002
ww	10577
zs	9968
```



## 练习

> 统计每个人三次考试成绩的最高分、三次考试成绩的平均分（保留2位小数）
>
> 源文件：`score.txt`

```
Bob 90 64 92
Alex 64 63 68
Grace 57 86 24
Henry 39 79 78
Adair 88 82 64
Chad 66 74 37
Colin 64 86 74
Eden 71 85 43
Grover 99 86 43
```



### Student类

```java
public class Student implements Writable {
    private String name = "";
    private List<Integer> score = new ArrayList<>();

    public Student() {}

    public String getName() {return name;}

    public void setName(String name) {this.name = name;}

    public List<Integer> getScore() {return score;}

    public void setScore(List<Integer> score) {this.score = score;}

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(name);
        out.writeInt(score.size());
        for (Integer integer : score) {
            out.writeInt(integer);
        }
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.name=in.readUTF();
        int size = in.readInt();
        this.score = new ArrayList<>(size);
        for (int i = 0;i<size;i++) {
            this.score.add(in.readInt());
        }
    }
}
```



### Mapper类

```java
public class ScoreMapper extends Mapper<LongWritable, Text,Text,Student> {

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(" ");
        Student s = new Student();
        List<Integer> list = new ArrayList<>();
        
        s.setName(arr[0]);
        list.add(Integer.valueOf(arr[1]));
        list.add(Integer.valueOf(arr[2]));
        list.add(Integer.valueOf(arr[3]));
        s.setScore(list);
        context.write(new Text(s.getName()),s);
    }
}
```



### 最高分Reducer类

```java
public class MaxScoreReducer extends Reducer<Text,Student,Text, IntWritable> {

    @Override
    protected void reduce(Text key, Iterable<Student> values, Context context) throws IOException, InterruptedException {
        int score = 0;
        List<Integer> list = new ArrayList<>();
        for (Student value : values) {
            list = value.getScore();
        }
        for (Integer integer : list) {
            if (score<integer) {
                score = integer;
            }
        }
        context.write(key,new IntWritable(score));
    }
}
```



### 平均分Reducer类

```java
public class AvgScoreReducer extends Reducer<Text,Student,Text, Text> {

    @Override
    protected void reduce(Text key, Iterable<Student> values, Context context) throws IOException, InterruptedException {
        List<Integer> list = new ArrayList<>();
        for (Student value : values) {
            list = value.getScore();
        }
        int score = 0;
        for (Integer integer : list) {
            score += integer;
        }

        DecimalFormat df = new DecimalFormat("0.00");
        double avgScore = score / list.size();
        String format = df.format(avgScore);

        context.write(key,new Text(format));
    }
}
```



### 最高分Driver类

```java
public class Driver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Job job = Job.getInstance(new Configuration());

        job.setJarByClass(Driver.class);
        job.setMapperClass(ScoreMapper.class);
        job.setReducerClass(MaxScoreReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Student.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path("hdfs://hadoop01:9000/txt/score.txt"));
        FileOutputFormat.setOutputPath(job, new Path("hdfs://hadoop01:9000/result/score3"));

        job.waitForCompletion(true);
    }
}

```



### 平均分Driver类

```java
public class Driver2 {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Job job = Job.getInstance(new Configuration());

        job.setJarByClass(Driver2.class);
        job.setMapperClass(ScoreMapper.class);
        job.setReducerClass(AvgScoreReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Student.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        FileInputFormat.addInputPath(job, new Path("hdfs://hadoop01:9000/txt/score.txt"));
        FileOutputFormat.setOutputPath(job, new Path("hdfs://hadoop01:9000/result/score4"));

        job.waitForCompletion(true);
    }
}
```



### 结果

> 最高分

```
Adair	88
Alex	68
Bob		92
Chad	74
Colin	86
Eden	85
Grace	86
Grover	99
Henry	79
```

> 平均分

```
Adair	78.00
Alex	65.00
Bob	82.00
Chad	59.00
Colin	74.67
Eden	66.33
Grace	55.67
Grover	76.00
Henry	65.33
```



# Partitioner - 分区

## 概述

1. 分区的作用是对数据进行分类
2. 在MapReduce中，分区默认是从0开始，依次递增
3. 在MapReduce中，每一个分区需要对应一个`ReduceTask`（Reduce产生的线程），每一个ReduceTask都会对应一个结果文件
4. 在MapReduce中，如果没有指定分区，那么默认使用的是`HashPartitioner`。如果不指定，默认ReduceTask的数量只有1个，所以只产生一个结果文件

## 案例

> 按地区，统计每一个人花费的总流量
>
> 源文件：flow.txt

```
13877779999 bj zs 2145
13766668888 sh ls 1028
13766668888 sh ls 9987
13877779999 bj zs 5678
13544445555 sz ww 10577
13877779999 sh zs 2145
13766668888 sh ls 9987
```

![](https://gitee.com/sxhDrk/images/raw/master/imgs/Partitioner分区案例结果.png)

### Flow类

```java
public class Flow implements Writable {

    private String phone = "";
    private String city = "";
    private String name = "";
    private int flow;

    public String getPhone() {return phone;}

    public void setPhone(String phone) {this.phone = phone;}

    public String getCity() {return city;}

    public void setCity(String city) {this.city = city;}

    public String getName() {return name;}

    public void setName(String name) {this.name = name;}

    public int getFlow() {return flow;}

    public void setFlow(int flow) {this.flow = flow;}

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(phone);
        out.writeUTF(city);
        out.writeUTF(name);
        out.writeInt(flow);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.phone = in.readUTF();
        this.city = in.readUTF();
        this.name = in.readUTF();
        this.flow = in.readInt();
    }
}
```



### Mapper类

```java
public class PartFlowMapper extends Mapper<LongWritable, Text, Text, Flow> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(" ");
        Flow f = new Flow();
        f.setPhone(arr[0]);
        f.setCity(arr[1]);
        f.setName(arr[2]);
        f.setFlow(Integer.parseInt(arr[3]));
        context.write(new Text(f.getName()), f);
    }
}
```



### Partitioner类

```java
// 按照城市对数据进行分类 - 不同的城市发往不同的分区
public class CityPartitioner extends Partitioner<Text, Flow> {
    @Override
    public int getPartition(Text text, Flow flow, int numPartitions) {
        // 获取城市
        String city = flow.getCity();
        // 分区是有编号的，编号是从0开始‘
        // 所以就需要将城市和编号进行对应
        if (city.equals("bj"))
            return 0;
        else if (city.equals("sh"))
            return 1;
        else
            return 2;
    }
}
```



### Reducer类

```java
public class PartFlowReducer extends Reducer<Text, Flow, Text, IntWritable> {
    @Override
    protected void reduce(Text key, Iterable<Flow> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (Flow val : values) {
            sum += val.getFlow();
        }
        context.write(key, new IntWritable(sum));
    }
}
```



### Driver类

```java
public class PartFlowDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(PartFlowDriver.class);
        job.setMapperClass(PartFlowMapper.class);
        job.setReducerClass(PartFlowReducer.class);

        // 指定分区类
        job.setPartitionerClass(CityPartitioner.class);
        // 每一个分区对应一个ReduceTask
        // 现在设置了3个分区，那么就需要设置3个ReduceTask
        job.setNumReduceTasks(3);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Flow.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job,
                new Path("hdfs://hadoop01:9000/txt/flow.txt"));
        FileOutputFormat.setOutputPath(job,
                new Path("hdfs://hadoop01:9000/result/partflow"));

        job.waitForCompletion(true);
    }
}
```



### 结果

> 生成了3个结果文件，00000，00001，00002

![](https://gitee.com/sxhDrk/images/raw/master/imgs/组件-Partitioner分区.png)

```sh
#00000
zs	7823
#00001
ls	21002
zs	2145
#00002
ww	10577
```





## 练习

> 按月份，统计每一个人的总成绩
>
> 源文件：目录score1

```sh
#chinese.txt
1 zhang 89
2 zhang 73
3 zhang 67
1 wang 49
2 wang 83
3 wang 27
1 li 77
2 li 66
3 li 89
#english.txt
1 zhang 55
2 zhang 69
3 zhang 75
1 wang 44
2 wang 64
3 wang 86
1 li 76
2 li 84
3 li 93
#math.txt
1 zhang 85
2 zhang 59
3 zhang 95
1 wang 74
2 wang 67
3 wang 96
1 li 45
2 li 76
3 li 67
```



### Score类

```java
public class Score implements Writable {

    private int month;
    private String name = "";
    private int score;

    public int getMonth() {return month;}

    public void setMonth(int month) {this.month = month;}

    public String getName() {return name;}

    public void setName(String name) {this.name = name;}

    public int getScore() {return score;}

    public void setScore(int score) {this.score = score;}

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeInt(month);
        out.writeUTF(name);
        out.writeInt(score);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.month = in.readInt();
        this.name = in.readUTF();
        this.score = in.readInt();
    }
}
```



### Mapper类

```java
public class PartScoreMapper extends Mapper<LongWritable, Text, Text, Score> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        // 拆分字段
        String[] arr = value.toString().split(" ");
        // 封装对象
        Score s = new Score();
        s.setMonth(Integer.parseInt(arr[0]));
        s.setName(arr[1]);
        s.setScore(Integer.parseInt(arr[2]));
        context.write(new Text(s.getName()), s);
    }
}
```



### Partitioner类

```java
public class MonthPartitioner extends Partitioner<Text, Score> {
    @Override
    public int getPartition(Text text, Score score, int numPartitions) {
        int month = score.getMonth();
        // 1月 -> 0号线程
        // 2月 -> 1号线程
        // 3月 -> 2号线程
        return month - 1;
    }
}
```



### Reducer类

```java
public class PartScoreReducer extends Reducer<Text, Score, Text, IntWritable> {
    @Override
    protected void reduce(Text key, Iterable<Score> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (Score val : values) {
            sum += val.getScore();
        }
        context.write(key, new IntWritable(sum));
    }
}
```



### Driver类

```java
public class PartScoreDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(PartScoreDriver.class);
        job.setMapperClass(PartScoreMapper.class);
        job.setReducerClass(PartScoreReducer.class);

        job.setPartitionerClass(MonthPartitioner.class);
        job.setNumReduceTasks(3);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Score.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job,
                new Path("hdfs://hadoop01:9000/txt/score1/"));
        FileOutputFormat.setOutputPath(job,
                new Path("hdfs://hadoop01:9000/result/partscore"));

        job.waitForCompletion(true);
    }
}

```



### 结果

```sh
#00000
li		198
wang	167
zhang	229
#00001
li		226
wang	214
zhang	201
#00002
li		249
wang	209
zhang	237
```





# Comparable - 排序

## 概述

1. 在MapReduce中，会默认对Mapper输出的==键==来进行==自然排序==，所以要求Mapper==输出的键==对应的类型必须实现Comparable接口。

   > 考虑到这个类型的对象还要被序列化，所以还要实现Writable接口，所以Hadoop提供了`WritableComparable`接口

2. 如果针对多字段进行排序，称之为二次排序

3. 在排序的时候，如果两个键`compareTo`的结果是0，那么在MapReduce中，会认为这两个键实际上是同一个键，然后在Reduce阶段将这两个键对应的值放到一组去

## 案例

> 对统计的总成绩进行降序排序
>
> 源文件：HDFS中 `/result/totalscore`

```
Adair	171
Alex	211
Bob		200
Chad	247
Colin	182
Eden	185
Grace	139
Grover	159
Henry	194
```



### Score类

```java
public class Score implements WritableComparable<Score> {

    private String name = "";
    private int score;

    public String getName() { return name;}

    public void setName(String name) {this.name = name;}

    public int getScore() {return score;}

    public void setScore(int score) {this.score = score;}

    // 按照分数进行降序排序
    @Override
    public int compareTo(Score o) {
        return o.score - this.score;
    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(name);
        out.writeInt(score);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.name = in.readUTF();
        this.score = in.readInt();
    }
}
```



### Mapper类

```java
public class SortScoreMapper extends Mapper<LongWritable, Text, Score, NullWritable> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split("\t");
        Score s = new Score();
        s.setName(arr[0]);
        s.setScore(Integer.parseInt(arr[1]));
        context.write(s, NullWritable.get());
    }
}
```



### Reducer类

```java
public class SortScoreReducer extends Reducer<Score, NullWritable, Text, IntWritable> {
    @Override
    protected void reduce(Score key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
        context.write(new Text(key.getName()), new IntWritable(key.getScore()));
    }
}
```



### Driver类

```java
public class SortScoreDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(SortScoreDriver.class);
        job.setMapperClass(SortScoreMapper.class);
        job.setReducerClass(SortScoreReducer.class);

        job.setMapOutputKeyClass(Score.class);
        job.setMapOutputValueClass(NullWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 在MapReduce中，如果一个文件以_开头，MapReduce默认不处理这个文件
        FileInputFormat.addInputPath(job,
                new Path("hdfs://hadoop01:9000/result/totalscore"));
        FileOutputFormat.setOutputPath(job,
                new Path("hdfs://hadoop01:9000/result/sortscore"));

        job.waitForCompletion(true);
    }
}
```



### 结果

> 将结果按照成绩降序排序

```
Chad	247
Alex	211
Bob		200
Henry	194
Eden	185
Colin	182
Adair	171
Grover	159
Grace	139
```



## 练习

> 先按照月份升序排序，如果是同一个月，按照利润降序
>
> 源文件：profit2.txt

```
2 tom 345
1 rose 235
1 tom 234
2 jim 572
3 rose 123
1 jim 321
2 tom 573
3 jim 876
3 tom 648
```



### Profit类

> 利润
>
> ==排序逻辑在compareTo方法中实现==

```java
public class Profit implements WritableComparable<Profit> {

    private int month;
    private String name;
    private int profit;

    public int getMonth() {return month;}

    public void setMonth(int month) {this.month = month;}

    public String getName() {return name;}

    public void setName(String name) {this.name = name;}

    public int getProfit() {return profit;}

    public void setProfit(int profit) {this.profit = profit;}

    @Override
    public String toString() {
        return "Profit{" +
                "month=" + month +
                ", name='" + name + '\'' +
                ", profit=" + profit +
                '}';
    }

    // 先按照月份排序，如果是同一个月，那么就按照例如降序
    @Override
    public int compareTo(Profit o) {
        int r1 = this.month - o.month;
        if (r1 == 0) {
            // 如果为0，说明是同一个月，同一个月要按照利润降序排序
            int r2 = o.profit - this.profit;
            return r2 == 0 ? 1 : r2;
        }
        return r1;
    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeInt(month);
        out.writeUTF(name);
        out.writeInt(profit);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.month = in.readInt();
        this.name = in.readUTF();
        this.profit = in.readInt();
    }
}
```



### Mapper类

```java
public class SortProfitMapper extends Mapper<LongWritable, Text, Profit, NullWritable> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(" ");
        Profit p = new Profit();
        p.setMonth(Integer.parseInt(arr[0]));
        p.setName(arr[1]);
        p.setProfit(Integer.parseInt(arr[2]));
        context.write(p, NullWritable.get());
    }
}
```



### Reducer类

```java
public class SortProfitReducer extends Reducer<Profit, NullWritable, Profit, NullWritable> {
    @Override
    protected void reduce(Profit key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
        context.write(key, NullWritable.get());
    }
}
```



### Driver类

```java
public class SortProfitDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(SortProfitDriver.class);
        job.setMapperClass(SortProfitMapper.class);
        job.setReducerClass(SortProfitReducer.class);

        job.setOutputKeyClass(Profit.class);
        job.setOutputValueClass(NullWritable.class);


        FileInputFormat.addInputPath(job,
                new Path("hdfs://hadoop01:9000/txt/profit3.txt"));
        FileOutputFormat.setOutputPath(job,
                new Path("hdfs://hadoop01:9000/result/sortprofit3"));

        job.waitForCompletion(true);
    }
}
```



### 结果

```
Profits{month=1, name='jim', profits=321}
Profits{month=1, name='rose', profits=235}
Profits{month=1, name='tom', profits=234}
Profits{month=2, name='tom', profits=573}
Profits{month=2, name='jim', profits=572}
Profits{month=2, name='tom', profits=345}
Profits{month=3, name='jim', profits=876}
Profits{month=3, name='tom', profits=648}
Profits{month=3, name='rose', profits=123}
```



# Combiner - 合并

1. 因为实际过程中MapTask的数量要远多于ReduceTask的数量，所以所有的计算压力最终都会落到ReduceTask上，所以就要考虑降低ReduceTask的计算压力

   > 考虑将ReduceTask一部分的计算前移，在MapTask这一端先进行一次汇总，最后ReduceTask再进行最后的汇总

2. Combiner的执行逻辑和Reducer是一样的，所以实际过程中将Reducer类也作为Combiner类来使用

3. Combiner的作用是为了减少数据总条数，不改变计算结果

4. 如果需要添加合并类，只需要在Driver中添加`job.setCombinerClass(xxxDriver.class)`即可

5. Combiner一定程度上确实可以提高效率，在实际开发中也建议尽量增加Combiner

   > 注意：并不是所有的场景都适合使用Combiner
   >
   > 例如：求和、去重、获取最值可以使用Combiner
   >
   > 求平均值等场景不适合用Combiner

![](https://gitee.com/sxhDrk/images/raw/master/imgs/Combiner-合并.png)



```java
public class SortProfitDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(SortProfitDriver.class);
        job.setMapperClass(SortProfitMapper.class);
        job.setReducerClass(SortProfitReducer.class);

        job.setOutputKeyClass(Profit.class);
        job.setOutputValueClass(NullWritable.class);

        // 指定合并类
        // job.setCombinerClass(SortProfitReducer.class);


        FileInputFormat.addInputPath(job,
                new Path("hdfs://hadoop01:9000/txt/profit3.txt"));
        FileOutputFormat.setOutputPath(job,
                new Path("hdfs://hadoop01:9000/result/sortprofit3"));

        job.waitForCompletion(true);
    }
}
```

