[TOC]

# 1、问题

> 之前系统连接的redis结构图
>
> easymall作为redis集群的客户端系统，通过Jedis代码连接3个节点，实现一致性hash的方法，完成分布式的使用逻辑

![](https://gitee.com/sxhDrk/images/raw/master/imgs/分布式下的redis结构图.png)

> 但是，一旦某一个redis节点由于故障等问题，导致数据不可用，就会影响整个系统的使用----->==说明当前结构不支持高可用==

![](https://gitee.com/sxhDrk/images/raw/master/imgs/Redis结构不支持高可用.png)

> ==举例==：用户登陆后，cookie对应的值信息存放在了6381节点中，如果6381节点宕机故障，就会导致系统无法继续使用6381中的数据，导致很多用户登录失效



# 2、解决思路

> 在每一个Redis节点后，添加子节点，用来备份数据。在主节点故障时，由子节点代替主节点工作

![](https://gitee.com/sxhDrk/images/raw/master/imgs/高可用-故障转移.png)

> 由此引出2个概念：

1. 主从数据备份：参考上图

   实现主从结构，在正在提供功能的主节点，引入从节点与之连接，同时数据一并备份，否则顶替主节点实现访问功能时，从节点不具备数据，相当于没顶替一样。

   ==所有与数据层相关的分布式集群技术中，只要是提到高可用，主从数据时基础==

2. 故障转移

   <img src="https://gitee.com/sxhDrk/images/raw/master/imgs/查看节点角色信息.png" style="zoom:50%;" />

   > 当客户端系统连接主节点实现数据的读写时，由于主节点宕机，客户端应该可以立即连接从节点继续处理数据的读写，这种思路----==故障转移==。



# 3、高可用的结构搭建

## 3.1、Redis的主从复制

> Redis支持主从关系（mysql也支持），而且支持一主多从，多级主从的结构
>
> ==根据经验，一般使用**一级主从**，每个节点最多挂接6个从节点。如果更复杂，会导致数据同步的故障，不稳定==

![](https://gitee.com/sxhDrk/images/raw/master/imgs/高可用-主从数据备份.png)





## 3.2、搭建主从配置

> 实现：==1主2从==的结构
>
> - 主节点：6382
> - 从节点：6383，6384

![](https://gitee.com/sxhDrk/images/raw/master/imgs/1主2从.png)





### 准备3个节点

1. 模板配置文件（redis.conf）拷贝3份

   ```c
   [root@10-9-104-184 redis-3.2.11]# cp redis.conf redis6382.conf
   [root@10-9-104-184 redis-3.2.11]# cp redis.conf redis6383.conf
   [root@10-9-104-184 redis-3.2.11]# cp redis.conf redis6384.conf
   ```

2. 每个文件修改成对应的端口号

   ```c
   :%s/6379/6382
   :%s/6379/6383
   :%s/6379/6384
   ```

3. 启动这3个节点

   ```c
   [root@10-9-104-184 redis-3.2.11]# redis-server redis6382.conf 
   [root@10-9-104-184 redis-3.2.11]# redis-server redis6383.conf 
   [root@10-9-104-184 redis-3.2.11]# redis-server redis6384.conf
   ```



### 查看每个进程服务端角色信息

```c
127.0.0.1:6384> info replication；
```

![](https://gitee.com/sxhDrk/images/raw/master/imgs/高可用-主从复制.png)

> 查看到当前节点的主从复制信息。
>
> 三个节点通过加载配置文件启动，==默认角色为master==，但是没有任何slave挂接



### 执行从节点的挂接

> 在登录的从节点客户端中，执行挂接到主节点的命令

==4.0以上更高版本使用：REPLICAOF host port 来进行从节点的挂接==

```shell
127.0.0.1:6383> slaveof 10.9.104.184 6382
127.0.0.1:6384> slaveof 10.9.104.184 6382
```

> 执行完毕挂接，`info replication`会看到该客户端已经从主节点角色变成从节点，同时6382多出来一个从节点的信息。

- 主节点

  ![](https://gitee.com/sxhDrk/images/raw/master/imgs/主从复制-测试数据同步.png)

- 从节点

  ![](https://gitee.com/sxhDrk/images/raw/master/imgs/主从复制-测试从节点能否写入数据.png)



### 测试主从结构

1. 测试数据同步

   - 在主节点添加数据

     ```c
     127.0.0.1:6382> set name wanglaoshi
     OK
     ```

   - 在从节点获取数据

     ```c
     127.0.0.1:6383> keys *
     1) "name"
         
     127.0.0.1:6384> keys *
     1) "name"
     ```

   ![](https://gitee.com/sxhDrk/images/raw/master/imgs/主从复制-测试主节点宕机后从节点的角色变化.png)

2. 测试从节点能否写入数据

   > 默认情况，slave节点是只读模式，防止连接错误，产生错误数据。一般情况下，不会开发可写的模式，都是只读。
   >
   > 从使用结构上，slave只能读数据。

   ![](https://gitee.com/sxhDrk/images/raw/master/imgs/主节点被挂接后.png)

3. 测试主节点宕机后，从节点的角色变化

   > 将主节点6382宕机之后，没有在从节点中发现角色的变化

   ![](https://gitee.com/sxhDrk/images/raw/master/imgs/从节点执行挂接后.png)



## 3.3、结论

支持高可用结构的基础，仅仅能够实现数据备份，不能完成故障转移。（主节点故障宕机后，从节点没有实现顶替）

故障转移：通过==哨兵进程==实现

↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓



# 4、哨兵高可用

## 4.1、哨兵进程

redis专门为主从结构高可用设计的特殊的服务端进程，不负责数据的处理，独立的进程。==只负责监听主从结构==

## 4.2、哨兵监听主从

### 结构

![](https://gitee.com/sxhDrk/images/raw/master/imgs/哨兵监听主从结构.png)

### 逻辑

1. 连接主节点，通过主节点info命令，获取主从所有节点状态信息

2. **启动监听所有节点的逻辑**。通过调用心跳检测（rpc协议），每秒钟都会发送心跳检测命令，到达主从所有节点。一旦发现超时未响应，就判断该节点宕机

3. **启动对事件投票机制**。对主节点宕机投票数==过半==，就会选举新的主节点。防止少数哨兵由于网络波动造成错误判断，一般不会配置一个哨兵进程作为监听逻辑的存在

   > 过半：
   >
   > - 3个哨兵，至少2个投票相同
   > - 4个哨兵，至少3个投票相同
   > - 5个哨兵，至少3个投票相同

4. **最终选举结束**。会将原有的主从结构重新进行发送命令的搭建。

   - 将宕机的主节点，挂接到新的主节点下
   - 将另外的从节点同样挂接到新的主节点下

   实现高可用的故障转移操作



# 5、搭建哨兵集群

## 5.1、搭建配置

> 启动哨兵进程需要配置文件，修改模板哨兵配置文件：`sentinel.con`
>
> ==配置3个哨兵进程，端口号分别为：26379，26380，26381。相同步骤修改3个哨兵进程==

### 关闭保护模式

> 17行：`protected-mode no`

![](https://gitee.com/sxhDrk/images/raw/master/imgs/哨兵配置文件-修改端口号.png)



### 修改端口号

> 21行：`port 26379`

![](https://gitee.com/sxhDrk/images/raw/master/imgs/哨兵配置文件-配置监听主节点.png)



### 配置监听主节点

> 69行：`sentinel monitor mymaster 196.128.16.84 6382 2`
>
> - `sentinel monitor `: 固定key值，表示哨兵监听的主节点内容
>
> - `mymaster` : 自定义的字符串，表示当前哨兵监听的主从结构的名称代号
>
> - `196.128.16.84 6382` : 主节点的ip地址和端口号
>
> - `2` : 主观下限票数。投票数量最少个数，由这个值来限制定义什么时候哨兵失效
>
>   比如：6个哨兵，主观下限票数为：4    表示有三个哨兵宕机，最大剩余票数为3，3<4，所以剩下的哨兵将失效，不起作用

![](https://gitee.com/sxhDrk/images/raw/master/imgs/哨兵配置文件-关闭保护模式.png)



## 5.2、启动哨兵进程

> 在==确定主从结构Redis启动正常==的前提下，启动3个哨兵进程

因为配置文件==没有开启后台进程==，所以开3个新的窗口，分别启动哨兵，观察运行时的日志输出





## 5.3、测试哨兵主从集群

1. **将主节点宕机处理**

   - 哨兵集群，由leader发起心跳检测，检测到宕机，发起投票，三个节点投票过半，选举新的主节点
   - 从原有从节点选择6384作为新的主节点
   - 将6382，6383重新挂接
   - 发现6382作为从节点时，检测到宕机状态

   ![](https://gitee.com/sxhDrk/images/raw/master/imgs/哨兵集群无法实现分布式.png)

2. **宕机的主节点重新启动**

   宕机的主节点恢复后，==哨兵记录中已经将该主节点记录为从节点角色==，哨兵登录到重启的主节点，执行挂接命令，将其转化为从节点



## 5.4、哨兵部分日志信息解释

```
· +reset-master <instance details>：主服务器已被重置。
· +slave <instance details>：一个新的从服务器已经被 Sentinel 识别并关联。
· +failover-state-reconf-slaves <instance details>：故障转移状态切换到了 reconf-slaves 状态。
· +failover-detected <instance details>：另一个 Sentinel 开始了一次故障转移操作，或者一个从服务器转换成了主服务器。
· +slave-reconf-sent <instance details>：领头（leader）的 Sentinel 向实例发送了 SLAVEOF 命令，为实例设置新的主服务器。
· +slave-reconf-inprog <instance details>：实例正在将自己设置为指定主服务器的从服务器，但相应的同步过程仍未完成。
· +slave-reconf-done <instance details>：从服务器已经成功完成对新主服务器的同步。
· -dup-sentinel <instance details>：对给定主服务器进行监视的一个或多个 Sentinel 已经因为重复出现而被移除 —— 当 Sentinel 实例重启的时候，就会出现这种情况。
· +sentinel <instance details>：一个监视给定主服务器的新 Sentinel 已经被识别并添加。
· +sdown <instance details>：给定的实例现在处于主观下线状态。
· -sdown <instance details>：给定的实例已经不再处于主观下线状态。
· +odown <instance details>：给定的实例现在处于客观下线状态。
· -odown <instance details>：给定的实例已经不再处于客观下线状态。
· +new-epoch <instance details>：当前的纪元（epoch）已经被更新。
· +try-failover <instance details>：一个新的故障迁移操作正在执行中，等待被大多数 Sentinel 选中（waiting to be elected by the majority）。
· +elected-leader <instance details>：赢得指定纪元的选举，可以进行故障迁移操作了。
· +failover-state-select-slave <instance details>：故障转移操作现在处于 select-slave 状态 —— Sentinel 正在寻找可以升级为主服务器的从服务器。
· no-good-slave <instance details>：Sentinel 操作未能找到适合进行升级的从服务器。Sentinel 会在一段时间之后再次尝试寻找合适的从服务器来进行升级，又或者直接放弃执行故障转移操作。
· selected-slave <instance details>：Sentinel 顺利找到适合进行升级的从服务器。
· failover-state-send-slaveof-noone <instance details>：Sentinel 正在将指定的从服务器升级为主服务器，等待升级功能完成。
· failover-end-for-timeout <instance details>：故障转移因为超时而中止，不过最终所有从服务器都会开始复制新的主服务器（slaves will eventually be configured to replicate with the new master anyway）。
· failover-end <instance details>：故障转移操作顺利完成。所有从服务器都开始复制新的主服务器了。
· +switch-master <master name> <oldip> <oldport> <newip> <newport>：配置变更，主服务器的 IP 和地址已经改变。 这是绝大多数外部用户都关心的信息。
· +tilt：进入 tilt 模式。
· -tilt：退出 tilt 模式。
```



# 6、哨兵集群Java代码测试

> 代码端要实现高可用：在技术端主节点正常情况下，代码客户端连接主节点操作读写，一旦主节点宕机，哨兵需要一段时间选举新主节点，选举完成后，代码端可以重新连接新的主节点，才能实现代码的高可用。
>
> jedis连接读写的主节点，必须通过哨兵集群才能实现，不能直接连接。因为哨兵管理着所有主从的相关信息。

## 6.1、代码测试

```java
@Test
public void test(){
    /*
     *	 jedis连接时，最终要想实现读写得连接上主节点，但是必须通过
     * 哨兵获取主节点信息，就可以实现故障转移之后，访问哨兵拿到最新的主节点信息
     * 提供哨兵的所有节点连接信息 26379 26380 26381，从中选择一个可连接的节点使用
     * 只要技术端哨兵集群有效，就可以完成信息发送和获取
     */
    Set<String> sentinelNodes=new HashSet<>();
    sentinelNodes.add(new HostAndPort("10.9.104.184",26379).toString());
    sentinelNodes.add(new HostAndPort("10.9.104.184",26380).toString());
    sentinelNodes.add(new HostAndPort("10.9.104.184",26381).toString());
    //创建哨兵的连接池对象
    JedisSentinelPool pool=new JedisSentinelPool("mymaster",sentinelNodes);//指定哨兵集群节点，传递主从代码
    //从连接池中可以获取很多集群信息
    HostAndPort chm = pool.getCurrentHostMaster();
    System.out.println(chm);//通过哨兵，获取当前现役master信息
    //通过哨兵获取master，就可以创建jedisPool对象操作主节点
    //JedisPool jPool=new JedisPool(new JedisPoolConfig(),chm.getHost(),chm.getPort());
    //操作主节点
    Jedis jedis = pool.getResource();//返回值就是当前现役master连接对象。
    jedis.set("name","王老师");
    System.out.println(jedis.get("name"));
}
```



## 6.2、结论：使用redis-cluster

> 哨兵集群的客户端代码逻辑很难实现分布式。
>
> 仅仅实现了方便快捷的主从高可用结构，但是只能维护一个分片的高可用，不能实现分布式。

![](https://gitee.com/sxhDrk/images/raw/master/imgs/哨兵更新新主节点日志记录解释.png)

> 解决思路：

把哨兵管理的主从看成一个整体数据，不能继续使用节点ip:port作为计算一致性hash的节点信息。重新封装客户端代码的一致性hash计算逻辑。

需要一种新的结构，==既能实现分布式，又能实现高可用==

- ==**redis-cluster**==